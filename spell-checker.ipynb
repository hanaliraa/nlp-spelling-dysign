{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, pandas as pd, string\n",
    "from nltk.metrics import edit_distance as ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {\n",
    "    'the': 1,\n",
    "    'beautiful': 2,\n",
    "    'ducks': 3,\n",
    "    'swam': 4,\n",
    "    'in': 5,\n",
    "    'big': 6,\n",
    "    'blue': 7,\n",
    "    'lake': 8,\n",
    "    'their': 9,\n",
    "    'feathers': 10,\n",
    "    'were': 11,\n",
    "    'very': 12,\n",
    "    'pretty': 13,\n",
    "    'and': 14,\n",
    "    'quite': 15,\n",
    "    'often': 16,\n",
    "    'would': 17,\n",
    "    'jump': 18,\n",
    "    'out': 19,\n",
    "    'of': 20,\n",
    "    'water': 21,\n",
    "}\n",
    "word_dict_key = list(word_dict.keys())\n",
    "sim_con = {\n",
    "    'b': ['d','h','q','p'],\n",
    "    'd': ['b', 'h', 'q','p'],\n",
    "    'h': ['b', 'd', 'q', 'p'],\n",
    "    'q': ['b', 'd', 'h', 'p'],\n",
    "    'p': ['b', 'd', 'h', 'q'],\n",
    "    'f': ['t'],\n",
    "    't': ['f'],\n",
    "}\n",
    "sim_vow = {\n",
    "    'o': ['u', 'e'],\n",
    "    'i': ['e'],\n",
    "    'a': ['e'],\n",
    "    'e': ['i', 'a', 'u']\n",
    "}\n",
    "consonants = ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'y', 'z']\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "# print(word_dict_key)\n",
    "def sim_characters(src, tgt):\n",
    "    score = []\n",
    "    phe = 0\n",
    "    unv = 0\n",
    "    aee = 0\n",
    "    doc = 0\n",
    "\n",
    "    # if length of src and tgt is same but the error is in character\n",
    "    if len(src) == len(tgt):\n",
    "        for c in range(len(src)):\n",
    "\n",
    "            if src[c] != tgt[c]:\n",
    "                # phonetic error [6]\n",
    "                if src[c] in sim_con.keys() and tgt[c] in sim_con[src[c]]: phe += 1\n",
    "\n",
    "                #unstressed vowel [10]\n",
    "                elif src[c] in sim_vow.keys() and tgt[c] in sim_vow[src[c]]: unv += 1\n",
    "\n",
    "    # if length of tgt is greater, meaning the child has written extra character\n",
    "    elif len(src) < len(tgt):\n",
    "        \n",
    "        prev_c = ''\n",
    "        for c in range(len(tgt)):\n",
    "\n",
    "            # adding extra e [1]\n",
    "            if c == len(tgt) - 1:\n",
    "                if tgt[-1] == 'e' and src[-1] != 'e': aee += 1\n",
    "\n",
    "            if c < len(src):\n",
    "                #phonetic error [6]\n",
    "                if src[c] in sim_con.keys() and tgt[c] in sim_con[src[c]]: phe += 1\n",
    "                \n",
    "                #doubling of consonants [3]\n",
    "                if src[c] != tgt[c] and tgt[c] == prev_c and tgt[c] in consonants: doc += 1\n",
    "                elif src[c] == tgt[c]: prev_c = src[c]\n",
    "\n",
    "    return phe, unv, aee, doc\n",
    "\n",
    "def scoring(df):\n",
    "\n",
    "    sentence_length = list()\n",
    "    PHE = []    #phonetic error [6]\n",
    "    UNV = []    #unstressed vowel [10]\n",
    "    AEE = []    #added extra e [1]\n",
    "    DOC = []    #doubling of consonants [3]\n",
    "\n",
    "    for ind, row in df.iterrows():\n",
    "        phe, unv, aee, doc = 0, 0, 0, 0\n",
    "        words = row['sentence'].translate(str.maketrans('','',string.punctuation)).lower()\n",
    "        words = words.split()\n",
    "        \n",
    "        #length of sentence\n",
    "        if len(words) == 25: sentence_length.append(0)\n",
    "        else: sentence_length.append(abs(len(words)-25))\n",
    "\n",
    "        for word in words:\n",
    "            src = ''\n",
    "            tgt = word\n",
    "            min_dist = 999\n",
    "\n",
    "            # edit distance to find the closest word if there are spelling errors\n",
    "            for x in word_dict_key:\n",
    "                dist = ed(word, x)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    src = x\n",
    "            if min_dist > 0:   # there is an error \n",
    "                a, b, c, d = sim_characters(src, tgt)\n",
    "                phe += a\n",
    "                unv += b\n",
    "                aee += c\n",
    "                doc += d\n",
    "            \n",
    "        PHE.append(phe)\n",
    "        UNV.append(unv)\n",
    "        AEE.append(aee)\n",
    "        DOC.append(doc)\n",
    "\n",
    "    df['sentence length'] = sentence_length\n",
    "    df['phonetic error'] = PHE\n",
    "    df['unstressed vowel'] = UNV\n",
    "    df['adding extra e'] = AEE\n",
    "    df['doubling consonant'] = DOC\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: there is error\n",
    "#0: no error\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df = scoring(df)\n",
    "df.to_csv('data.csv', index=False)\n",
    "print(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pos_label=1 is not a valid label. It should be one of ['ld', 'nt']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive - Habib University\\Kaavish\\nlp-spelling-dysign\\spell-checker.ipynb Cell 5\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Habib%20University/Kaavish/nlp-spelling-dysign/spell-checker.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Calculate evaluation metrics for the SVM classifier\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Habib%20University/Kaavish/nlp-spelling-dysign/spell-checker.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m svm_accuracy \u001b[39m=\u001b[39m accuracy_score(y_test, svm_predictions)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Habib%20University/Kaavish/nlp-spelling-dysign/spell-checker.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m svm_precision \u001b[39m=\u001b[39m precision_score(y_test, svm_predictions)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Habib%20University/Kaavish/nlp-spelling-dysign/spell-checker.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m svm_recall \u001b[39m=\u001b[39m recall_score(y_test, svm_predictions)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Habib%20University/Kaavish/nlp-spelling-dysign/spell-checker.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m svm_f1 \u001b[39m=\u001b[39m f1_score(y_test, svm_predictions)\n",
      "File \u001b[1;32mc:\\Users\\Digital Traders\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1954\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprecision_score\u001b[39m(\n\u001b[0;32m   1826\u001b[0m     y_true,\n\u001b[0;32m   1827\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1833\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1834\u001b[0m ):\n\u001b[0;32m   1835\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   1836\u001b[0m \n\u001b[0;32m   1837\u001b[0m \u001b[39m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1952\u001b[0m \u001b[39m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   1953\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1954\u001b[0m     p, _, _, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   1955\u001b[0m         y_true,\n\u001b[0;32m   1956\u001b[0m         y_pred,\n\u001b[0;32m   1957\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1958\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[0;32m   1959\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[0;32m   1960\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mprecision\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[0;32m   1961\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1962\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[0;32m   1963\u001b[0m     )\n\u001b[0;32m   1964\u001b[0m     \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mc:\\Users\\Digital Traders\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[0;32m   1575\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Digital Traders\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1382\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1380\u001b[0m     \u001b[39mif\u001b[39;00m pos_label \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m present_labels:\n\u001b[0;32m   1381\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(present_labels) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m-> 1382\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1383\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpos_label=\u001b[39m\u001b[39m{\u001b[39;00mpos_label\u001b[39m}\u001b[39;00m\u001b[39m is not a valid label. It \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshould be one of \u001b[39m\u001b[39m{\u001b[39;00mpresent_labels\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m     labels \u001b[39m=\u001b[39m [pos_label]\n\u001b[0;32m   1387\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: pos_label=1 is not a valid label. It should be one of ['ld', 'nt']"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Drop the 'id' column\n",
    "data = data.drop('id', axis=1)\n",
    "data = data.drop('sentence', axis=1)\n",
    "data = data.drop('label', axis=1)\n",
    "\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data.drop('class', axis=1)\n",
    "y = data['class']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an SVM classifier\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set using the SVM classifier\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics for the SVM classifier\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "svm_precision = precision_score(y_test, svm_predictions)\n",
    "svm_recall = recall_score(y_test, svm_predictions)\n",
    "svm_f1 = f1_score(y_test, svm_predictions)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "logreg_classifier = LogisticRegression()\n",
    "logreg_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set using the logistic regression classifier\n",
    "logreg_predictions = logreg_classifier.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics for the logistic regression classifier\n",
    "logreg_accuracy = accuracy_score(y_test, logreg_predictions)\n",
    "logreg_precision = precision_score(y_test, logreg_predictions)\n",
    "logreg_recall = recall_score(y_test, logreg_predictions)\n",
    "logreg_f1 = f1_score(y_test, logreg_predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"SVM Classifier:\")\n",
    "print(\"Accuracy:\", svm_accuracy)\n",
    "print(\"Precision:\", svm_precision)\n",
    "print(\"Recall:\", svm_recall)\n",
    "print(\"F1 Score:\", svm_f1)\n",
    "print()\n",
    "print(\"Logistic Regression Classifier:\")\n",
    "print(\"Accuracy:\", logreg_accuracy)\n",
    "print(\"Precision:\", logreg_precision)\n",
    "print(\"Recall:\", logreg_recall)\n",
    "print(\"F1 Score:\", logreg_f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
